{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "![OER Header](img/header.png)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Biomedical Data – Lesson 3: Irrelevant Features\n",
    "\n",
    "Part of the *Microcredit Synthetic Biomedical Data*.\n",
    "\n",
    "➡️ [Back to Lesson 3a: Irrelevant Features (Noise)](03a_irrelevant_features_noise.ipynb)\n",
    "➡️ [Module Overview (README)](../README.md)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap from Lesson 3a\n",
    "You:\n",
    "- Generated synthetic biomedical datasets with informative and noise features.\n",
    "- Visualized feature distributions and class separability.\n",
    "- Evaluated cohen's d effect size for informative features and noise features.\n",
    "- Understood how effect size ranking is influenced by feature dimension and sample size.\n",
    "\n",
    "Now we focus on the effect of the dimension and distribution of **irrelevant features** — variables that carry *no* useful information about the label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If too many irrelevant features are present, they can:\n",
    "- Make it harder to identify true biomarkers. Too many irrelevant features can hide the true signal.\n",
    "- Increase model complexity and training time and make feature selection more difficult.\n",
    "- Lead to overfitting if models start learning patterns in the noise.\n",
    "\n",
    "Synthetic data allows us to deliberately add irrelevant features to test how models and methods cope with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "In this lesson, we will add irrelevant features to a synthetic dataset\n",
    "and see how they effect model performance and feature correlations. After completing this notebook, you will be able to:\n",
    "- Add purely irrelevant features with configurable distributions (normal, uniform, Laplace).\n",
    "- Visualize the distributions of irrelevant features.\n",
    "- Evaluate how increasing the number of irrelevant features affects model performance using cross-validated balanced accuracy.\n",
    "- Analyze spurious correlations between irrelevant features and the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Code – Imports, Installation/Upgrade"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from biomedical_data_generator import ClassConfig, DatasetConfig, generate_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "# Configure plotting for better readability\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data configuration from irrelevant features lesson"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_noise_features = 1200  # number of irrelevant features to add\n",
    "n_class_samples = 15  # samples per class, analogous to a small biomedical study\n",
    "cfg_noise_normal = DatasetConfig(\n",
    "    n_informative=8,\n",
    "    n_noise=n_noise_features,\n",
    "    class_configs=[\n",
    "        ClassConfig(n_samples=n_class_samples),  # two balanced classes\n",
    "        ClassConfig(n_samples=n_class_samples),\n",
    "    ],\n",
    "    class_sep=[1.2],  # keep consistent with the baseline\n",
    "    prefixed_feature_naming=True,  # i1..i8, n1..n12\n",
    "    noise_distribution=\"normal\",\n",
    "    random_state=1908,\n",
    ")\n",
    "xn, yn, meta_n = generate_dataset(cfg_noise_normal, return_dataframe=True)\n",
    "\n",
    "# Combine X and y for plotting\n",
    "x_noise = xn.copy()\n",
    "x_noise.insert(0, \"class\", yn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Inspect noise distributions\n",
    "## Normal noise\n",
    "With `noise_distribution=\"normal\"` and `noise_scale=1.0`, we expect:\n",
    "- Approximately zero mean and unit variance for noise columns.\n",
    "- No visible class separation in the noise columns.\n",
    "- Similar spread across noise columns, up to sampling variability.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.histplot(data=x_noise, x=\"n42\", hue=\"class\", kde=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Comparison of informative features alone versus the same features with additional noise features:\")\n",
    "# informative features only\n",
    "X_tr1, X_te1, y_tr1, y_te1 = train_test_split(xn.iloc[:, :8].values, yn, test_size=0.3, random_state=42, stratify=yn)\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=42))\n",
    "clf.fit(X_tr1, y_tr1)\n",
    "\n",
    "pred = clf.predict(X_te1)\n",
    "print(\"Logistic Regression balanced accuracy (informative only):\", round(balanced_accuracy_score(y_te1, pred), 3))\n",
    "\n",
    "# added 1200 noise features\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(xn.values, yn, test_size=0.3, random_state=42, stratify=yn)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=42))\n",
    "clf.fit(X_tr, y_tr)\n",
    "pred = clf.predict(X_te)\n",
    "print(\n",
    "    \"Logistic Regression balanced accuracy (with added noise features):\",\n",
    "    round(balanced_accuracy_score(y_te, pred), 3),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform noise\n",
    "Now we switch to `noise_distribution=\"uniform\"`.\n",
    "Here, low and high set the bounds of the interval, so values are uniformly distributed in `[low, high]`. We use noise_distribution_params={\"low\": -2, \"high\": 2}, so noise features will mostly lie in `[-2, 2]` with a flat (uniform) density."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cfg_noise_uniform = DatasetConfig(\n",
    "    n_informative=8,\n",
    "    n_noise=1200,\n",
    "    class_configs=[\n",
    "        ClassConfig(n_samples=5000),  # two balanced classes\n",
    "        ClassConfig(n_samples=5000),  # set a larger sample size for better empirical estimates\n",
    "    ],\n",
    "    noise_distribution=\"uniform\",\n",
    "    noise_distribution_params={\"low\": -2, \"high\": 2},\n",
    "    random_state=42,\n",
    ")\n",
    "xu, yu, meta_u = generate_dataset(cfg_noise_uniform, return_dataframe=True)\n",
    "xu.insert(0, \"class\", yu)\n",
    "\n",
    "# Empirical min/max of first 5 noise columns to verify ~[-2, 2]\n",
    "mins = xu[[f\"n{k}\" for k in range(1, 6)]].min().round(3)\n",
    "maxs = xu[[f\"n{k}\" for k in range(1, 6)]].max().round(3)\n",
    "display(pd.DataFrame({\"min\": mins, \"max\": maxs}))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.histplot(data=xu, x=\"n2\", hue=\"class\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform noise expectations\n",
    "\n",
    "With `noise_distribution=\"uniform\"` and `noise_scale=2.0`, values are roughly in **`[-2, 2]`**.\n",
    "You should observe:\n",
    "- Empirical **min/max** close to −2 and +2 (but not equal—finite samples fall short of true bounds).\n",
    "- **Nearly constant density** within the interval, unlike the bell shape of normal noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - The influence of noise on model performance\n",
    "## Quick train/test split on the normal-noise dataset to see if noise hurts performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Comparison of informative features alone versus the same features with additional noise features:\")\n",
    "# informative features only\n",
    "X_tr1, X_te1, y_tr1, y_te1 = train_test_split(xn.iloc[:, :8].values, yn, test_size=0.3, random_state=42, stratify=yn)\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=42))\n",
    "clf.fit(X_tr1, y_tr1)\n",
    "\n",
    "pred = clf.predict(X_te1)\n",
    "print(\"Logistic Regression balanced accuracy (informative only):\", round(balanced_accuracy_score(y_te1, pred), 3))\n",
    "\n",
    "# added 1200 noise features\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(xn.values, yn, test_size=0.3, random_state=42, stratify=yn)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=42))\n",
    "clf.fit(X_tr, y_tr)\n",
    "pred = clf.predict(X_te)\n",
    "print(\n",
    "    \"Logistic Regression balanced accuracy (8 informative with 1200 noise features added):\",\n",
    "    round(balanced_accuracy_score(y_te, pred), 3),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 — Balanced accuracy vs. number of noise features (cross-validated)\n",
    "\n",
    "We now study how *increasing the number of irrelevant (noise) features* affects model performance. We measure how a simple linear classifier (Logistic Regression) behaves under **Stratified 5-Fold CV** as we increase the number of standard normal distributed noise features.\n",
    "\n",
    "**Plan**\n",
    "1. Keep the true signal fixed: 8 informative features, same effect size/separability.\n",
    "2. Vary the number of **pure noise** features: e.g. `k ∈ {0, 50, 200, 1000}`.\n",
    "3. Train a simple linear classifier (Logistic Regression) **with scaling** and evaluate via **Stratified 5-Fold CV**. We keep the model fixed to isolate the effect of dimensionality.\n",
    "4. Only one factor changes: the number of irrelevant features.\n",
    "5. Track: **Balanced accuracy** (robust to small class imbalances)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment configuration\n",
    "\n",
    "# Noise counts to sweep over\n",
    "NOISE_COUNTS = [0, 50, 200, 1000, 10000]\n",
    "\n",
    "# Estimator: scale -> logistic regression (binary)\n",
    "# liblinear is robust for small-sample, L2 by default\n",
    "ESTIMATOR = make_pipeline(RobustScaler(), LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=2000))\n",
    "\n",
    "CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "SCORER = make_scorer(balanced_accuracy_score)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why logistic regression + scaling?\n",
    "\n",
    "- **Logistic Regression** is a transparent linear baseline; with L2 regularization it copes reasonably with moderate noise.\n",
    "- **RobustScaler** avoids feature-scale dominance and reduces optimization issues. Robust scaling (median + IQR) is less sensitive to outliers than standard scaling (mean + std).\n",
    "- The pipeline ensures **no data leakage**: scaling parameters are learned **inside** each CV fold.\n",
    "- Alternatives to try later:\n",
    "  - **Tree ensembles** (e.g. Random Forests) for non-linear baselines.\n",
    "  - **Elastic Net** for embedded sparsity (tuning needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guarding against leakage in CV\n",
    "\n",
    "By wrapping preprocessing and the estimator in a **single pipeline** and passing it to `cross_val_score`, we guarantee that:\n",
    "- **Scaling** and **model fitting** occur **only on the training folds**.\n",
    "- The held-out fold remains unseen until scoring, yielding **honest** performance estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CV evaluation helper\n",
    "def evaluate_cv(x_df: pd.DataFrame, y_np: np.ndarray, estimator=ESTIMATOR, cv=CV, scorer=SCORER) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Returns (mean_bal_acc, std_bal_acc) from K-fold CV.\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(estimator, x_df, y_np, cv=cv, scoring=scorer, n_jobs=None)\n",
    "    return float(scores.mean()), float(scores.std())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Run the sweep over noise counts, collecting balanced accuracy"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_sweep(noise_counts, noise_dist, noise_distribution_params) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for k_noise in noise_counts:\n",
    "        # Build a dataset with a fixed informative features, samples, class separation and seed but variable\n",
    "        # number of pure-noise features\n",
    "        fixed_cfg = DatasetConfig(\n",
    "            n_noise=k_noise,\n",
    "            noise_distribution=noise_dist,\n",
    "            noise_distribution_params=noise_distribution_params,\n",
    "            n_informative=8,\n",
    "            class_configs=[\n",
    "                ClassConfig(n_samples=15),\n",
    "                ClassConfig(n_samples=15),\n",
    "            ],\n",
    "            class_sep=[1.2],\n",
    "            random_state=42,\n",
    "        )\n",
    "        x_k, y_k, meta_k = generate_dataset(fixed_cfg, return_dataframe=True)\n",
    "        mean_ba, std_ba = evaluate_cv(x_k, y_k)\n",
    "        results.append(\n",
    "            {\n",
    "                \"dist\": str(noise_dist),\n",
    "                \"scale\": noise_distribution_params,\n",
    "                \"n_noise\": k_noise,\n",
    "                \"n_informative\": len(meta_k.informative_idx),\n",
    "                \"bal_acc_mean\": mean_ba,\n",
    "                \"bal_acc_std\": std_ba,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(results).sort_values(\"n_noise\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "result_df = run_sweep(NOISE_COUNTS, \"normal\", {\"loc\": 0, \"scale\": 1.0})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What trends to expect\n",
    "\n",
    "**Balanced accuracy** typically **decreases** as `n_noise` increases (harder signal recovery)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Balanced accuracy vs number of noise features\n",
    "plt.figure()\n",
    "plt.plot(result_df[\"n_noise\"], result_df[\"bal_acc_mean\"], marker=\"o\")\n",
    "plt.title(\"Balanced accuracy vs number of noise features\")\n",
    "plt.xlabel(\"Number of normal distributed noise features\")\n",
    "plt.ylabel(\"Balanced accuracy (CV mean)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the plots carefully\n",
    "\n",
    "- Expect **non-monotonic fluctuations** due to finite-sample randomness—don’t over-fit to a single curve.\n",
    "- Look for the **overall slope**: does performance degrade roughly as dimensionality rises?\n",
    "- If a method looks unusually stable, verify you are **not leaking information** and that regularization is properly tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational notes\n",
    "\n",
    "- This sweep is lightweight (single model, 5-fold CV).\n",
    "- As you add methods (e.g., nested CV, repeated subsampling, multiple models), runtime will grow quickly.\n",
    "- Keep random seeds fixed for **comparability**, and cache intermediate results when exploring many settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation checklist\n",
    "\n",
    "- **Performance drop**: As `n_noise` grows, balanced accuracy should typically **decrease** unless regularization and sample size are strong.\n",
    "- **Sample size matters**: With fixed effect size, increasing `n_samples` generally improves both performance and discovery metrics.\n",
    "- **Regularization**: Linear models with proper regularization can resist some noise, but the *curse of dimensionality* still applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biomedical context: why this matters\n",
    "\n",
    "Omics datasets (e.g., gene expression, proteomics, methylation) often have **p ≫ n**:\n",
    "- Thousands of features with **few dozens of samples**.\n",
    "- Many features are unrelated to the phenotype—i.e., **irrelevant** for prediction.\n",
    "\n",
    "Understanding how irrelevant features affect **generalization** and **feature discovery** helps you:\n",
    "- Choose appropriate **regularization** and **model families**,\n",
    "- Design **feature selection** protocols that avoid false discoveries,\n",
    "- Plan for **validation** on independent cohorts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 — Comparing noise distributions\n",
    "\n",
    "Repeat the sweep with:\n",
    "\n",
    "1. `noise_distribution=\"uniform\", noise_distribution_params={\"low\": -2, \"high\": 2}`\n",
    " >**Uniform** noise (bounded support) usually yields tighter extremes than normal at small `n`, reducing outlier risks.\n",
    "\n",
    "2. `noise_distribution=\"laplace\", noise_distribution_params={\"loc\": 0, \"scale\": 1.0}`\n",
    ">**Laplace** noise (heavier tails) increases the chance of **extreme values**, which can inflate spurious correlations and destabilize rankings.\n",
    "\n",
    "When sweeping distributions, keep **`n_samples`, `n_informative`, and `class_sep`** fixed to isolate the distributional effect. Then compare curves (balanced accuracy) across distributions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_uniform = run_sweep(NOISE_COUNTS, \"uniform\", {\"low\": -2, \"high\": 2})\n",
    "df_laplace = run_sweep(NOISE_COUNTS, \"laplace\", {\"loc\": 0, \"scale\": 1.0})\n",
    "display(df_uniform, df_laplace)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Balanced accuracy vs number of uniform noise features\n",
    "plt.figure()\n",
    "plt.plot(df_uniform[\"n_noise\"], df_uniform[\"bal_acc_mean\"], marker=\"o\")\n",
    "plt.title(\"Balanced accuracy vs number of noise features\")\n",
    "plt.xlabel(\"Number of noise features\")\n",
    "plt.ylabel(\"Balanced accuracy (CV mean)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Balanced accuracy vs number of laplace noise features\n",
    "plt.figure()\n",
    "plt.plot(df_laplace[\"n_noise\"], df_laplace[\"bal_acc_mean\"], marker=\"o\")\n",
    "plt.title(\"Balanced accuracy vs number of noise features\")\n",
    "plt.xlabel(\"Number of noise features\")\n",
    "plt.ylabel(\"Balanced accuracy (CV mean)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "**Model performance** (if you ran the optional classifier):\n",
    "  - How much did adding noise affect classification accuracy?\n",
    "  - What might happen if you add *hundreds* of noise features?\n",
    "\n",
    "**Think ahead**:\n",
    "In real biomedical studies, many measured variables are irrelevant.\n",
    "Why is it important to detect and filter them before applying complex models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Spurious correlations\n",
    "\n",
    "Irrelevant features should be **uncorrelated** with `y` (up to sampling noise) and show no systematic correlation with informative features.\n",
    "\n",
    "We’ll compute:\n",
    "- Absolute correlation between each **noise** feature and the **label** `y`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Work on the first 20 features of the initial noise dataset (xn, yn, meta_n)\n",
    "y_numeric = pd.Series(yn, name=\"y\").astype(float)\n",
    "\n",
    "# 1) Correlation of the first 20 features with the label\n",
    "y_corr = xn.iloc[:, :20].apply(lambda s: s.corr(y_numeric)).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 of the first 20 features (8 informative, 12 noise)\\n|abs(corr(features, y))|:\")\n",
    "display(y_corr.to_frame().head(10).round(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Work on the full initial noise dataset (xn, yn, meta_n)\n",
    "y_numeric = pd.Series(yn, name=\"y\").astype(float)\n",
    "print(f\"Number of features: {xn.shape[1]}, Number of samples: {xn.shape[0]}\\n\")\n",
    "\n",
    "# 1) Correlation of features with the label\n",
    "y_corr = xn.apply(lambda s: s.corr(y_numeric)).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 |abs(corr(features, y))|:\")\n",
    "display(y_corr.to_frame().head(10).round(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spurious correlations grow with dimensionality\n",
    "\n",
    "Even if each noise feature is uninformative, the **largest absolute correlation** among many noise variables can be non-trivial **by chance**.\n",
    "\n",
    "Rule of thumb (for intuition): the maximum spurious correlation scales roughly like\n",
    "$$\n",
    "\\max_j \\lvert r_{j,y}\\rvert \\sim \\sqrt{\\tfrac{2\\log p}{n}}\n",
    "$$\n",
    "where \\(p\\) is the number of features and \\(n\\) the sample size.\n",
    "\n",
    "\n",
    "➡️ As you add more noise features or reduce sample size, **chance correlations** become more prominent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Takeaway\n",
    "- **Irrelevant features** add no predictive value and can **dilute** signal. Synthetic data allows us to control how much noise exists, so we can test how methods behave in high-noise vs. low-noise scenarios.\n",
    "- As the number of irrelevant features increases, model performance (e.g., balanced accuracy) typically **decreases** due to the *curse of dimensionality*.\n",
    "- Different noise distributions (normal, uniform, Laplace) have distinct characteristics that can affect model training and evaluation.\n",
    "- Spurious correlations between noise features and the label can arise by chance, especially as dimensionality increases.\n",
    "- In high-noise settings, feature selection (see MC XXX) becomes crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In **Lesson 3c: Correlated Features**, you will:\n",
    "- Simulate features that **move together** (e.g., biological pathways).\n",
    "- Visualize correlation structures.\n",
    "- See how **redundancy** complicates analysis.\n",
    "\n",
    "➡️ Continue with: **[`03c_correlated_features.ipynb`](03c_correlated_features.ipynb)`**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
